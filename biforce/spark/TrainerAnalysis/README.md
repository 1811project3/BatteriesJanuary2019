![Biforce Brand](https://github.com/revaturelabs/biforce/blob/dev/images/biforce-brand.png)

## Description
Our batch (dev1901) was tasked with performing data analysis focused around Trainer data to determine trainer's strengths, weaknesses, and any cause for concerns with a trainer. To do this, we created spark jobs to process and aggregate data to be sent to Amazon Redshift and visualized through Metabase.

## Objectives
Spark team set out to accomplish the following goals: extract data from Amazon RDS into Hive to transform tables into a workable format for spark jobs to read and process, write Spark code through Java to evaluate data and write to CSV files to be moved into the Amazon S3 bucket, move files saved in Amazon S3 to Amazon Redshift for Metabase to read and visualize for non-technical users.

Spark jobs will be looking to accomplish the following goals: extract one-on-one scores from trainers and normalize the scores to represent how hard a trainer is during one-on-ones (give the users an idea of what kind of scores to expect from a specific trainer), determine how proficient a trainer is at teaching a specific subject, show how often trainers submit grades and catch those who fall behind in grade submission, throw red flags (cause for concern) on trainers that are statistically underperforming compared to the rest of the trainers at Revature, build percentile ranges across Revature to show where trainees should score on average in a specific subject for a specific type of assessment (in terms of grades).

## Implementation
General Implementation: General workflow begins with extracting the data from the given Amazon RDS into Hive for processing. We used Apache Sqoop to move data between the RDS database and Hive using the following command line prompt: `sqoop import-all-tables -Dhadoop.security.credential.provider.path=jceks://hdfs/user/root/caliber.password.jceks --connect (jdbc url of RDS) --username (RDS Username) --password (RDS Password) --hive-import --hive-database (name of database in Hive) --hive-drop-import-delims`.

Once the data is in Hive, HiveQL queries where ran to transform the data into views to be written locally. These views are used for Spark to read and perform computer processing. Refer to the `HiveSQL_Scripts` directory to view and use the scripts to transform data. *NOTE: For ease of use in the future, all tables are written to a CSV file delimited by ~. This delimiter makes reading the CSV's much easier when importing to Amazon Redshift.*

Spark code was written in Java using the SparkSQL libraries to load in schema and process data through query language. A singular .jar file holding multiple Driver classes is used to run the spark jobs either locally or through the EMR cluster provided by Revature. Syntax for running a spark job through command line: `spark-submit --class com.revature.TrainerAnalysis.(name of driver class) Biforce_Trainer_Analysis.jar (location of the input files to be processed)`. *NOTE: The end command of each Spark job writes a CSV file delimited by ~ to a specified output directory. The current version writes to a local directory within a Cloudera Virtual Machine. To write directly to S3, open the driver classes and comment out the output string line and uncomment the appropriate string with the S3 bucket location. Save the files and rebuild the jar using mvn package.*

Code within Spark can write to the S3 bucket directly, but for the duration of our project, we had to write the CSV file output locally then manually make the transition to S3 then Redshift (this process is automated in the Oozie workflow by the Oozie team). If also running locally and forced to move manually, install the Amazon CLI to your machine and use the command `aws s3 cp (name of file to be copied to S3) s3://(name of S3 bucket)`.

Our team used DBeaver6.0, a free multi-platform database tool provided at https://dbeaver.io, to access and modify the Amazon Redshift database provided by Revature. Within Redshift, we first had to develop a schema to load the tables stored in S3. Once the schema was written into Redshift, tables saved in Redshift were then loaded in using the following command: `copy [table name in redshift] from 's3://[database]/[table]' access_key_id '[access key]' secret_access_key '[secret access key]' REGION '[region]' DELIMITER '[delimiter]';`.

One-on-One Scores: To normalize trainer one-on-one scores, we decided to use the grades of a trainer's students that were denoted "Verbal, non-QC" scores, take the max, min, and average of all of their scores, then calculate the z-score of that trainer's trainee scores. This produces a standardized number between 0 and 1. This number can be compared to other trainers to see just how much harder a particular trainer scores their trainees as well as obtain an idea of what scores should you hope to expect from a trainee for a given trainer.

Topic Proficiency: To judge a trainer's proficiency at training, we used the QC evaluation scores to evaluate a trainer's trainees in a specific subject. Those scores were weighted following this logic: Poor=10, Average=50, Good=70, Superstar=100. These scores are then averaged grouped by trainer and subject. This allows a user to see a weighted score of how well a trainer is able to have their trainees score in a given subject.

Grade Submission: We calculated grade submission data per batch. Grades from exam and verbal submissions were tracked. By subtracting 1 from the last week number QC submitted a grade, we can determine how many weeks a trainer should have submitted a grade for their batch. If any exam or verbal grade was submitted during a particular week (since exams are given weekly), it's inferred that grades are submitted for that batch for that week. The difference between the number of inferred weeks grades were submitted and the QC submitted grades minus 1 gives us the number of weeks where grades were not submitted.

Statistical Red Flags: We decided to throw red flags by looking at the results of the grade submissions and topic proficiency and singling out trainers that are underperforming to a level of cause for concern. This job takes the information from the proficiency job and grade submission job, and pulls trainers that either have a proficiency level below a standard deviation from the mean (falls below 84% of the other trainers at Revature) or trainers that have missed submitting grades 2 or more times throughout their batch.

Percentile Ranges: To build a range of where a trainer's trainees are scoring across Revature for a specific subject and assessment type, we developed bell curves using the standard deviation and mean of all grades grouped by subject and assessment type.

## Current Issues
General Implementation: The biggest issue our iteration ran into was that our jobs failed to write to S3 through Spark, thus everything had to be ran locally and written locally before being manually moved to S3. This process is automated through Oozie if ran on an EMR cluster, but for the purpose of local testing, everything was manual. Dependencies can be downloaded and passed to the `spark-submit` through the `--jars` flag, and this bypasses the S3a exception we ran into, but it then wouldn't accept the key and ID for access to the S3 bucket. This will be extremely problematic when run on a production scale and should be addressed as soon as possible.

One-on-One Scores: The logic used to calculate the normalized scores can get extremely skewed for the first 2-3 weeks due to some trainers receiving strong programmers from the beginning and other trainers getting very novice programmers. Due to this, trainers that get weaker trainees to begin with are immediately punished in terms of our logic by receiving low scores for those initial weeks. Possible solutions would be to either omit the first week or two, or omit trainees that are still in training and use only finished batches.

Topic Proficiency: The biggest issue comes from attempting to calculate QC scores through weights. By putting QC assessment to a number, it fails to account for a score that may have been on the edge of passing into a higher or lower score (a trainee that scored an "Average" but could have been right on the edge of receiving a "Good" score will be solely calculated as a 50, when maybe it should be more like a 68). This causes some inaccuracy to judging a trainer's proficiency this way. Two solutions to look into would be to either adjust the weights to possibly give more accurate results, or use some other method to judge proficiency all together (the way data was given to us limits our options though). 

Grade Submission: Logic for this problem breaks with the data we were given to work with. It was inferred that the data had been tampered with in previous iterations, limiting our accuracy and consistancy with the logic. Working off of a newer dataset would more accurately depict how well our logic works.

Statistical Red Flags: Issues with the red flags being thrown stem from issues with the other jobs this job pulls data from. Please refer to the Topic Proficiency and Grade Submission issues for details.

Percentile Ranges: Much like calculating the one-on-one scores, alot of the issues are caused by earlier weeks and dropped students that had consistantly scored extremely poorly causing outliers that skew the ranges to one side. Filtering out these outliers in the data, or simply working with data where the trainees have already completed training can alleviate this issue.
